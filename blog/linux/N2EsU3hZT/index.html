<!doctype html><html lang=zh dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content='
使用deviceQuery获取显卡信息


  环境
  #

Linux、nvidia、cuda

  命令
  #

/usr/local/cuda/extras/demo_suite/deviceQuery

 CUDA Device Query (Runtime API) version (CUDART static linking)

Detected 2 CUDA Capable device(s)

Device 0: "GeForce RTX 2080 Ti"
  CUDA Driver Version / Runtime Version          10.1 / 10.1
  CUDA Capability Major/Minor version number:    7.5
  Total amount of global memory:                 11019 MBytes (11554717696 bytes)
  (68) Multiprocessors, ( 64) CUDA Cores/MP:     4352 CUDA Cores
  GPU Max Clock rate:                            1545 MHz (1.54 GHz)
  Memory Clock rate:                             7000 Mhz
  Memory Bus Width:                              352-bit
  L2 Cache Size:                                 5767168 bytes
  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)
  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers
  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers
  Total amount of constant memory:               65536 bytes
  Total amount of shared memory per block:       49152 bytes
  Total number of registers available per block: 65536
  Warp size:                                     32
  Maximum number of threads per multiprocessor:  1024
  Maximum number of threads per block:           1024
  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)
  Maximum memory pitch:                          2147483647 bytes
  Texture alignment:                             512 bytes
  Concurrent copy and kernel execution:          Yes with 3 copy engine(s)
  Run time limit on kernels:                     No
  Integrated GPU sharing Host Memory:            No
  Support host page-locked memory mapping:       Yes
  Alignment requirement for Surfaces:            Yes
  Device has ECC support:                        Disabled
  Device supports Unified Addressing (UVA):      Yes
  Device supports Compute Preemption:            Yes
  Supports Cooperative Kernel Launch:            Yes
  Supports MultiDevice Co-op Kernel Launch:      Yes
  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0
  Compute Mode:
     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >

Device 1: "GeForce RTX 2080 Ti"
  CUDA Driver Version / Runtime Version          10.1 / 10.1
  CUDA Capability Major/Minor version number:    7.5
  Total amount of global memory:                 11019 MBytes (11554717696 bytes)
  (68) Multiprocessors, ( 64) CUDA Cores/MP:     4352 CUDA Cores
  GPU Max Clock rate:                            1545 MHz (1.54 GHz)
  Memory Clock rate:                             7000 Mhz
  Memory Bus Width:                              352-bit
  L2 Cache Size:                                 5767168 bytes
  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)
  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers
  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers
  Total amount of constant memory:               65536 bytes
  Total amount of shared memory per block:       49152 bytes
  Total number of registers available per block: 65536
  Warp size:                                     32
  Maximum number of threads per multiprocessor:  1024
  Maximum number of threads per block:           1024
  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)
  Maximum memory pitch:                          2147483647 bytes
  Texture alignment:                             512 bytes
  Concurrent copy and kernel execution:          Yes with 3 copy engine(s)
  Run time limit on kernels:                     No
  Integrated GPU sharing Host Memory:            No
  Support host page-locked memory mapping:       Yes
  Alignment requirement for Surfaces:            Yes
  Device has ECC support:                        Disabled
  Device supports Unified Addressing (UVA):      Yes
  Device supports Compute Preemption:            Yes
  Supports Cooperative Kernel Launch:            Yes
  Supports MultiDevice Co-op Kernel Launch:      Yes
  Device PCI Domain ID / Bus ID / location ID:   0 / 2 / 0
  Compute Mode:
     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >
> Peer access from GeForce RTX 2080 Ti (GPU0) -> GeForce RTX 2080 Ti (GPU1) : Yes
> Peer access from GeForce RTX 2080 Ti (GPU1) -> GeForce RTX 2080 Ti (GPU0) : Yes

deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 10.1, CUDA Runtime Version = 10.1, NumDevs = 2

  源码
  #

https://github.com/NVIDIA/cuda-samples/tree/master/Samples/deviceQuery'><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://notes.zhangwenbing.com/blog/linux/N2EsU3hZT/"><meta property="og:site_name" content="张文兵的笔记"><meta property="og:title" content="linux nvidia 获取显卡信息"><meta property="og:description" content=' 使用deviceQuery获取显卡信息
环境 # Linux、nvidia、cuda
命令 # /usr/local/cuda/extras/demo_suite/deviceQuery CUDA Device Query (Runtime API) version (CUDART static linking) Detected 2 CUDA Capable device(s) Device 0: "GeForce RTX 2080 Ti" CUDA Driver Version / Runtime Version 10.1 / 10.1 CUDA Capability Major/Minor version number: 7.5 Total amount of global memory: 11019 MBytes (11554717696 bytes) (68) Multiprocessors, ( 64) CUDA Cores/MP: 4352 CUDA Cores GPU Max Clock rate: 1545 MHz (1.54 GHz) Memory Clock rate: 7000 Mhz Memory Bus Width: 352-bit L2 Cache Size: 5767168 bytes Maximum Texture Dimension Size (x,y,z) 1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384) Maximum Layered 1D Texture Size, (num) layers 1D=(32768), 2048 layers Maximum Layered 2D Texture Size, (num) layers 2D=(32768, 32768), 2048 layers Total amount of constant memory: 65536 bytes Total amount of shared memory per block: 49152 bytes Total number of registers available per block: 65536 Warp size: 32 Maximum number of threads per multiprocessor: 1024 Maximum number of threads per block: 1024 Max dimension size of a thread block (x,y,z): (1024, 1024, 64) Max dimension size of a grid size (x,y,z): (2147483647, 65535, 65535) Maximum memory pitch: 2147483647 bytes Texture alignment: 512 bytes Concurrent copy and kernel execution: Yes with 3 copy engine(s) Run time limit on kernels: No Integrated GPU sharing Host Memory: No Support host page-locked memory mapping: Yes Alignment requirement for Surfaces: Yes Device has ECC support: Disabled Device supports Unified Addressing (UVA): Yes Device supports Compute Preemption: Yes Supports Cooperative Kernel Launch: Yes Supports MultiDevice Co-op Kernel Launch: Yes Device PCI Domain ID / Bus ID / location ID: 0 / 1 / 0 Compute Mode: < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) > Device 1: "GeForce RTX 2080 Ti" CUDA Driver Version / Runtime Version 10.1 / 10.1 CUDA Capability Major/Minor version number: 7.5 Total amount of global memory: 11019 MBytes (11554717696 bytes) (68) Multiprocessors, ( 64) CUDA Cores/MP: 4352 CUDA Cores GPU Max Clock rate: 1545 MHz (1.54 GHz) Memory Clock rate: 7000 Mhz Memory Bus Width: 352-bit L2 Cache Size: 5767168 bytes Maximum Texture Dimension Size (x,y,z) 1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384) Maximum Layered 1D Texture Size, (num) layers 1D=(32768), 2048 layers Maximum Layered 2D Texture Size, (num) layers 2D=(32768, 32768), 2048 layers Total amount of constant memory: 65536 bytes Total amount of shared memory per block: 49152 bytes Total number of registers available per block: 65536 Warp size: 32 Maximum number of threads per multiprocessor: 1024 Maximum number of threads per block: 1024 Max dimension size of a thread block (x,y,z): (1024, 1024, 64) Max dimension size of a grid size (x,y,z): (2147483647, 65535, 65535) Maximum memory pitch: 2147483647 bytes Texture alignment: 512 bytes Concurrent copy and kernel execution: Yes with 3 copy engine(s) Run time limit on kernels: No Integrated GPU sharing Host Memory: No Support host page-locked memory mapping: Yes Alignment requirement for Surfaces: Yes Device has ECC support: Disabled Device supports Unified Addressing (UVA): Yes Device supports Compute Preemption: Yes Supports Cooperative Kernel Launch: Yes Supports MultiDevice Co-op Kernel Launch: Yes Device PCI Domain ID / Bus ID / location ID: 0 / 2 / 0 Compute Mode: < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) > > Peer access from GeForce RTX 2080 Ti (GPU0) -> GeForce RTX 2080 Ti (GPU1) : Yes > Peer access from GeForce RTX 2080 Ti (GPU1) -> GeForce RTX 2080 Ti (GPU0) : Yes deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 10.1, CUDA Runtime Version = 10.1, NumDevs = 2 源码 # https://github.com/NVIDIA/cuda-samples/tree/master/Samples/deviceQuery'><meta property="og:locale" content="zh"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-06-04T06:40:16+08:00"><meta property="article:modified_time" content="2020-06-04T06:40:16+08:00"><meta property="article:tag" content="Linux"><title>linux nvidia 获取显卡信息 | 张文兵的笔记</title>
<link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png><link rel=canonical href=https://notes.zhangwenbing.com/blog/linux/N2EsU3hZT/><link rel=stylesheet href=/book.min.7bc0bb908f28131671779537cc0c91a39075ed4cba93497f05888d37375171ce.css integrity="sha256-e8C7kI8oExZxd5U3zAyRo5B17Uy6k0l/BYiNNzdRcc4=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/zh.search.min.210aa5fca8aef8461939aafd1f59c5ca5f752cf863a04c07586f0cc5165c0b65.js integrity="sha256-IQql/Kiu+EYZOar9H1nFyl91LPhjoEwHWG8MxRZcC2U=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>张文兵的笔记</span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=搜索 aria-label=搜索 maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li><input type=checkbox id=section-6d1804e163fb7f5fbc576b94719a59cb class=toggle>
<label for=section-6d1804e163fb7f5fbc576b94719a59cb class="flex justify-between"><a role=button>👨🏼‍💻 编程相关</a></label><ul><li><input type=checkbox id=section-8ec8d24621de2d29aa3aa48e5e4147bf class=toggle>
<label for=section-8ec8d24621de2d29aa3aa48e5e4147bf class="flex justify-between"><a role=button>📔 C/C++</a></label><ul><li><input type=checkbox id=section-3436919d500fd2bb71607f1799c530e9 class=toggle>
<label for=section-3436919d500fd2bb71607f1799c530e9 class="flex justify-between"><a role=button>🔖 MacOS</a></label><ul><li><a href=/notes/2024/06/06/sf4ff6x8cuyhvzougj9dwg/>📝 MacOS签名</a></li><li><a href=/notes/2024/05/28/3txbbb7iruppjsg4p7rwnb/>📝 MacOS堆栈大小</a></li></ul></li><li><input type=checkbox id=section-9c529f10c7b539e237ae1f42d47b905f class=toggle>
<label for=section-9c529f10c7b539e237ae1f42d47b905f class="flex justify-between"><a role=button>🔖 QtWidget</a></label><ul><li><a href=/notes/2025/05/13/gfegatfpustuecdeaifds7/>📝 defer实现</a></li><li><a href=/notes/2025/05/13/6dhpyx7jyybzrnvw7qremy/>📝 异步更新UI</a></li></ul></li><li><input type=checkbox id=section-3be7c6476ab3ef4abcb7bd8a29aca7ca class=toggle>
<label for=section-3be7c6476ab3ef4abcb7bd8a29aca7ca class="flex justify-between"><a role=button>🔖 QtQml</a></label><ul><li><a href=/notes/2024/08/01/rnc4n7cfifidx8ca2rhjme/>📝 QML鼠标事件</a></li><li><a href=/notes/2024/08/06/ny773erqjgbzmzgfikfvds/>📝 QML虚拟列表</a></li></ul></li><li><input type=checkbox id=section-543c46644dbaa17c070d1fbbe6f809c8 class=toggle>
<label for=section-543c46644dbaa17c070d1fbbe6f809c8 class="flex justify-between"><a role=button>🔖 build</a></label><ul><li><a href=/notes/2025/04/28/akhjvrsank3gkyirsrxgtc/>📝 运行是崩溃</a></li><li><a href=/notes/2025/04/30/fqog2lfxmepcxbbyzucxeq/>📝 内存重叠</a></li></ul></li><li><input type=checkbox id=section-7ef42c94e3a59c5cb77c843c093effb8 class=toggle>
<label for=section-7ef42c94e3a59c5cb77c843c093effb8 class="flex justify-between"><a role=button>🔖 Python</a></label><ul><li><a href=/notes/2025/02/10/7auahqg8nnxm5ydhxzqrnr/>📝 调用Python代码</a></li><li><a href=/notes/2025/02/10/gqci9riugfetribe6j6fou/>📝 调用Python方法</a></li><li><a href=/notes/2025/02/10/pkdrvn9md3ggfmbxahx4ef/>📝 注册Python模块</a></li></ul></li></ul></li><li><input type=checkbox id=section-aea9addc7ea0cf58a2c2c9ec52f959c9 class=toggle>
<label for=section-aea9addc7ea0cf58a2c2c9ec52f959c9 class="flex justify-between"><a role=button>📔 Git</a></label><ul><li><a href=/notes/2024/03/11/j3c62fv8dglvf2kg975zlv/>📝 标签操作</a></li><li><a href=/notes/2024/03/14/5kqlzqjqawbxf2t6vusxar/>📝 删除Commit</a></li><li><a href=/blog/git/WNYzxj6GR/>📝 git之删除远程分支</a></li><li><a href=/blog/git/-fZeBsu3T/>📝 git更新.gitignore</a></li><li><a href=/blog/git/H1Krv6b-4/>📝 git修改历史cimmit信息</a></li><li><a href=/blog/git/By9OI51xN/>📝 git基础笔记</a></li><li><a href=/blog/git/Hk6GGp6cQ/>📝 Git回滚到某个commit</a></li></ul></li></ul></li><li><input type=checkbox id=section-caf3b688bb818cc6cfc82165db78eba5 class=toggle>
<label for=section-caf3b688bb818cc6cfc82165db78eba5 class="flex justify-between"><a role=button>♻️ 回收归档</a></label><ul><li><input type=checkbox id=section-4a72fedb3d7dfc18dbbcab83511bf75a class=toggle>
<label for=section-4a72fedb3d7dfc18dbbcab83511bf75a class="flex justify-between"><a role=button>📔 Rust</a></label><ul><li><input type=checkbox id=section-8aa26b33a0315c1be70924160dd1a92c class=toggle>
<label for=section-8aa26b33a0315c1be70924160dd1a92c class="flex justify-between"><a role=button>🔖 学习笔记</a></label><ul><li><a href=/notes/2024/03/06/e3ksq3zkmps14gcxuirwzx/>📝 所有权</a></li><li><a href=/notes/2024/03/06/o4arkqgfnqygr5ckif2gkk/>📝 借用与引用</a></li></ul></li></ul></li></ul></li><li><a href=/links/>🤝 友情链接</a></li></ul><ul><li><a href=/articles/>📚 我的文章</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu>
</label><strong>linux nvidia 获取显卡信息</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#环境>环境</a></li><li><a href=#命令>命令</a></li><li><a href=#源码>源码</a><ul><li><a href=#编译>编译</a></li></ul></li></ul></nav></aside></header><article class="markdown book-post"><h1 class=book-post-title><a href=/blog/linux/N2EsU3hZT/>linux nvidia 获取显卡信息</a></h1><div class=book-post-meta><span class=book-post-meta-date>2020-06-04</span>
<span class=book-post-meta-separator-categories>&nbsp; | &nbsp;</span>
<span class=book-post-meta-categories><a href=/categories/linux/>Linux</a>
</span><span class=book-post-meta-separator-tags>&nbsp; | &nbsp;</span>
<span class=book-post-meta-tags><a href=/tags/linux/>Linux</a></span></div><blockquote><p>使用deviceQuery获取显卡信息</p></blockquote><h2 id=环境>环境
<a class=anchor href=#%e7%8e%af%e5%a2%83>#</a></h2><p>Linux、nvidia、cuda</p><h2 id=命令>命令
<a class=anchor href=#%e5%91%bd%e4%bb%a4>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>/usr/local/cuda/extras/demo_suite/deviceQuery
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span> CUDA Device Query <span style=color:#f92672>(</span>Runtime API<span style=color:#f92672>)</span> version <span style=color:#f92672>(</span>CUDART static linking<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Detected <span style=color:#ae81ff>2</span> CUDA Capable device<span style=color:#f92672>(</span>s<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Device 0: <span style=color:#e6db74>&#34;GeForce RTX 2080 Ti&#34;</span>
</span></span><span style=display:flex><span>  CUDA Driver Version / Runtime Version          10.1 / 10.1
</span></span><span style=display:flex><span>  CUDA Capability Major/Minor version number:    7.5
</span></span><span style=display:flex><span>  Total amount of global memory:                 <span style=color:#ae81ff>11019</span> MBytes <span style=color:#f92672>(</span><span style=color:#ae81ff>11554717696</span> bytes<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>(</span>68<span style=color:#f92672>)</span> Multiprocessors, <span style=color:#f92672>(</span> 64<span style=color:#f92672>)</span> CUDA Cores/MP:     <span style=color:#ae81ff>4352</span> CUDA Cores
</span></span><span style=display:flex><span>  GPU Max Clock rate:                            <span style=color:#ae81ff>1545</span> MHz <span style=color:#f92672>(</span>1.54 GHz<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>  Memory Clock rate:                             <span style=color:#ae81ff>7000</span> Mhz
</span></span><span style=display:flex><span>  Memory Bus Width:                              352-bit
</span></span><span style=display:flex><span>  L2 Cache Size:                                 <span style=color:#ae81ff>5767168</span> bytes
</span></span><span style=display:flex><span>  Maximum Texture Dimension Size <span style=color:#f92672>(</span>x,y,z<span style=color:#f92672>)</span>         1D<span style=color:#f92672>=(</span>131072<span style=color:#f92672>)</span>, 2D<span style=color:#f92672>=(</span>131072, 65536<span style=color:#f92672>)</span>, 3D<span style=color:#f92672>=(</span>16384, 16384, 16384<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>  Maximum Layered 1D Texture Size, <span style=color:#f92672>(</span>num<span style=color:#f92672>)</span> layers  1D<span style=color:#f92672>=(</span>32768<span style=color:#f92672>)</span>, <span style=color:#ae81ff>2048</span> layers
</span></span><span style=display:flex><span>  Maximum Layered 2D Texture Size, <span style=color:#f92672>(</span>num<span style=color:#f92672>)</span> layers  2D<span style=color:#f92672>=(</span>32768, 32768<span style=color:#f92672>)</span>, <span style=color:#ae81ff>2048</span> layers
</span></span><span style=display:flex><span>  Total amount of constant memory:               <span style=color:#ae81ff>65536</span> bytes
</span></span><span style=display:flex><span>  Total amount of shared memory per block:       <span style=color:#ae81ff>49152</span> bytes
</span></span><span style=display:flex><span>  Total number of registers available per block: <span style=color:#ae81ff>65536</span>
</span></span><span style=display:flex><span>  Warp size:                                     <span style=color:#ae81ff>32</span>
</span></span><span style=display:flex><span>  Maximum number of threads per multiprocessor:  <span style=color:#ae81ff>1024</span>
</span></span><span style=display:flex><span>  Maximum number of threads per block:           <span style=color:#ae81ff>1024</span>
</span></span><span style=display:flex><span>  Max dimension size of a thread block <span style=color:#f92672>(</span>x,y,z<span style=color:#f92672>)</span>: <span style=color:#f92672>(</span>1024, 1024, 64<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>  Max dimension size of a grid size    <span style=color:#f92672>(</span>x,y,z<span style=color:#f92672>)</span>: <span style=color:#f92672>(</span>2147483647, 65535, 65535<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>  Maximum memory pitch:                          <span style=color:#ae81ff>2147483647</span> bytes
</span></span><span style=display:flex><span>  Texture alignment:                             <span style=color:#ae81ff>512</span> bytes
</span></span><span style=display:flex><span>  Concurrent copy and kernel execution:          Yes with <span style=color:#ae81ff>3</span> copy engine<span style=color:#f92672>(</span>s<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>  Run time limit on kernels:                     No
</span></span><span style=display:flex><span>  Integrated GPU sharing Host Memory:            No
</span></span><span style=display:flex><span>  Support host page-locked memory mapping:       Yes
</span></span><span style=display:flex><span>  Alignment requirement <span style=color:#66d9ef>for</span> Surfaces:            Yes
</span></span><span style=display:flex><span>  Device has ECC support:                        Disabled
</span></span><span style=display:flex><span>  Device supports Unified Addressing <span style=color:#f92672>(</span>UVA<span style=color:#f92672>)</span>:      Yes
</span></span><span style=display:flex><span>  Device supports Compute Preemption:            Yes
</span></span><span style=display:flex><span>  Supports Cooperative Kernel Launch:            Yes
</span></span><span style=display:flex><span>  Supports MultiDevice Co-op Kernel Launch:      Yes
</span></span><span style=display:flex><span>  Device PCI Domain ID / Bus ID / location ID:   <span style=color:#ae81ff>0</span> / <span style=color:#ae81ff>1</span> / <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>  Compute Mode:
</span></span><span style=display:flex><span>     &lt; Default <span style=color:#f92672>(</span>multiple host threads can use ::cudaSetDevice<span style=color:#f92672>()</span> with device simultaneously<span style=color:#f92672>)</span> &gt;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Device 1: <span style=color:#e6db74>&#34;GeForce RTX 2080 Ti&#34;</span>
</span></span><span style=display:flex><span>  CUDA Driver Version / Runtime Version          10.1 / 10.1
</span></span><span style=display:flex><span>  CUDA Capability Major/Minor version number:    7.5
</span></span><span style=display:flex><span>  Total amount of global memory:                 <span style=color:#ae81ff>11019</span> MBytes <span style=color:#f92672>(</span><span style=color:#ae81ff>11554717696</span> bytes<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>(</span>68<span style=color:#f92672>)</span> Multiprocessors, <span style=color:#f92672>(</span> 64<span style=color:#f92672>)</span> CUDA Cores/MP:     <span style=color:#ae81ff>4352</span> CUDA Cores
</span></span><span style=display:flex><span>  GPU Max Clock rate:                            <span style=color:#ae81ff>1545</span> MHz <span style=color:#f92672>(</span>1.54 GHz<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>  Memory Clock rate:                             <span style=color:#ae81ff>7000</span> Mhz
</span></span><span style=display:flex><span>  Memory Bus Width:                              352-bit
</span></span><span style=display:flex><span>  L2 Cache Size:                                 <span style=color:#ae81ff>5767168</span> bytes
</span></span><span style=display:flex><span>  Maximum Texture Dimension Size <span style=color:#f92672>(</span>x,y,z<span style=color:#f92672>)</span>         1D<span style=color:#f92672>=(</span>131072<span style=color:#f92672>)</span>, 2D<span style=color:#f92672>=(</span>131072, 65536<span style=color:#f92672>)</span>, 3D<span style=color:#f92672>=(</span>16384, 16384, 16384<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>  Maximum Layered 1D Texture Size, <span style=color:#f92672>(</span>num<span style=color:#f92672>)</span> layers  1D<span style=color:#f92672>=(</span>32768<span style=color:#f92672>)</span>, <span style=color:#ae81ff>2048</span> layers
</span></span><span style=display:flex><span>  Maximum Layered 2D Texture Size, <span style=color:#f92672>(</span>num<span style=color:#f92672>)</span> layers  2D<span style=color:#f92672>=(</span>32768, 32768<span style=color:#f92672>)</span>, <span style=color:#ae81ff>2048</span> layers
</span></span><span style=display:flex><span>  Total amount of constant memory:               <span style=color:#ae81ff>65536</span> bytes
</span></span><span style=display:flex><span>  Total amount of shared memory per block:       <span style=color:#ae81ff>49152</span> bytes
</span></span><span style=display:flex><span>  Total number of registers available per block: <span style=color:#ae81ff>65536</span>
</span></span><span style=display:flex><span>  Warp size:                                     <span style=color:#ae81ff>32</span>
</span></span><span style=display:flex><span>  Maximum number of threads per multiprocessor:  <span style=color:#ae81ff>1024</span>
</span></span><span style=display:flex><span>  Maximum number of threads per block:           <span style=color:#ae81ff>1024</span>
</span></span><span style=display:flex><span>  Max dimension size of a thread block <span style=color:#f92672>(</span>x,y,z<span style=color:#f92672>)</span>: <span style=color:#f92672>(</span>1024, 1024, 64<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>  Max dimension size of a grid size    <span style=color:#f92672>(</span>x,y,z<span style=color:#f92672>)</span>: <span style=color:#f92672>(</span>2147483647, 65535, 65535<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>  Maximum memory pitch:                          <span style=color:#ae81ff>2147483647</span> bytes
</span></span><span style=display:flex><span>  Texture alignment:                             <span style=color:#ae81ff>512</span> bytes
</span></span><span style=display:flex><span>  Concurrent copy and kernel execution:          Yes with <span style=color:#ae81ff>3</span> copy engine<span style=color:#f92672>(</span>s<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>  Run time limit on kernels:                     No
</span></span><span style=display:flex><span>  Integrated GPU sharing Host Memory:            No
</span></span><span style=display:flex><span>  Support host page-locked memory mapping:       Yes
</span></span><span style=display:flex><span>  Alignment requirement <span style=color:#66d9ef>for</span> Surfaces:            Yes
</span></span><span style=display:flex><span>  Device has ECC support:                        Disabled
</span></span><span style=display:flex><span>  Device supports Unified Addressing <span style=color:#f92672>(</span>UVA<span style=color:#f92672>)</span>:      Yes
</span></span><span style=display:flex><span>  Device supports Compute Preemption:            Yes
</span></span><span style=display:flex><span>  Supports Cooperative Kernel Launch:            Yes
</span></span><span style=display:flex><span>  Supports MultiDevice Co-op Kernel Launch:      Yes
</span></span><span style=display:flex><span>  Device PCI Domain ID / Bus ID / location ID:   <span style=color:#ae81ff>0</span> / <span style=color:#ae81ff>2</span> / <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>  Compute Mode:
</span></span><span style=display:flex><span>     &lt; Default <span style=color:#f92672>(</span>multiple host threads can use ::cudaSetDevice<span style=color:#f92672>()</span> with device simultaneously<span style=color:#f92672>)</span> &gt;
</span></span><span style=display:flex><span>&gt; Peer access from GeForce RTX <span style=color:#ae81ff>2080</span> Ti <span style=color:#f92672>(</span>GPU0<span style=color:#f92672>)</span> -&gt; GeForce RTX <span style=color:#ae81ff>2080</span> Ti <span style=color:#f92672>(</span>GPU1<span style=color:#f92672>)</span> : Yes
</span></span><span style=display:flex><span>&gt; Peer access from GeForce RTX <span style=color:#ae81ff>2080</span> Ti <span style=color:#f92672>(</span>GPU1<span style=color:#f92672>)</span> -&gt; GeForce RTX <span style=color:#ae81ff>2080</span> Ti <span style=color:#f92672>(</span>GPU0<span style=color:#f92672>)</span> : Yes
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>deviceQuery, CUDA Driver <span style=color:#f92672>=</span> CUDART, CUDA Driver Version <span style=color:#f92672>=</span> 10.1, CUDA Runtime Version <span style=color:#f92672>=</span> 10.1, NumDevs <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span>
</span></span></code></pre></div><h2 id=源码>源码
<a class=anchor href=#%e6%ba%90%e7%a0%81>#</a></h2><p><a href=https://github.com/NVIDIA/cuda-samples/tree/master/Samples/deviceQuery>https://github.com/NVIDIA/cuda-samples/tree/master/Samples/deviceQuery</a></p><h3 id=编译>编译
<a class=anchor href=#%e7%bc%96%e8%af%91>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>/usr/local/cuda/bin/nvcc deviceQuery.cpp -I /usr/local/cuda/samples/common/inc/ -o deviceQuery
</span></span></code></pre></div></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#环境>环境</a></li><li><a href=#命令>命令</a></li><li><a href=#源码>源码</a><ul><li><a href=#编译>编译</a></li></ul></li></ul></nav></div></aside></main><footer class=page-footer><p><span id=page-footer-time>载入中...</span>
<script>function secondToDate(e){if(!e)return 0;var t=new Array(0,0,0,0,0);return e>=365*24*3600&&(t[0]=parseInt(e/(365*24*3600)),e%=365*24*3600),e>=24*3600&&(t[1]=parseInt(e/(24*3600)),e%=24*3600),e>=3600&&(t[2]=parseInt(e/3600),e%=3600),e>=60&&(t[3]=parseInt(e/60),e%=60),e>0&&(t[4]=e),t}</script><script type=text/javascript language=javascript>function setTime(){var e=Math.round(new Date(Date.UTC(2016,11,1,0,0,0)).getTime()/1e3),t=Math.round(((new Date).getTime()+8*60*60*1e3)/1e3);currentTime=secondToDate(t-e),currentTimeHtml=currentTime[0]+"年"+currentTime[1]+"天"+currentTime[2]+"时"+currentTime[3]+"分"+currentTime[4]+"秒",document.getElementById("page-footer-time").innerHTML="网站运行:"+currentTimeHtml}setInterval(setTime,1e3)</script></p></footer></body></html>